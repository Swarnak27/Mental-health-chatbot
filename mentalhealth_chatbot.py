# -*- coding: utf-8 -*-
"""FineTuneGPT3.5 with mental health GPT dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TLYJo6lZokjE9NCokPU8yuHbTlGaxa5W
"""



!pip install transformers==4.28.0
import pandas as pd
from transformers import GPT2LMHeadModel, GPT2TokenizerFast
from transformers import TextDataset, DataCollatorForLanguageModeling
from transformers import Trainer, TrainingArguments

!pip install datasets

!pip install transformers==4.28.0
!pip install Datasets
import pandas as pd
from transformers import  GPT2LMHeadModel,AutoTokenizer, GPT2TokenizerFast
from transformers import DataCollatorForLanguageModeling
from transformers import Trainer, TrainingArguments
from datasets import Dataset

# Sample question answering data as a list of dictionaries
qa_data = [
    {"question": "What is addiction?", "answer": "you commit with some problems"},
    {"question": "Who wrote the play 'Romeo and Juliet'?", "answer": "William Shakespeare"},
    # Add more question-answer pairs here
]

# Create a DataFrame from the question answering data
df = pd.DataFrame(qa_data)

# Save the DataFrame to a CSV file
file_path = "question_answers.csv"
df.to_csv(file_path, index=False)  # Set index=False to exclude row numbers in the CSV

# Provide the file path of the CSV file
file_path = "question_answers.csv"

# Read the CSV file into a DataFrame
df = pd.read_csv(file_path)

# If needed, you can convert the DataFrame back to a list of dictionaries
qa_data = df.to_dict(orient='records')

# Now you have your question answering data in the 'qa_data' list of dictionaries.
# You can use it for your question answering tasks.

# Load the pre-trained GPT-2 model and tokenizer
model_name = "gpt2"
tokenizer = GPT2TokenizerFast.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

tokenizer.pad_token = tokenizer.eos_token

# Configure and train the model using the Trainer class
training_args = TrainingArguments(
    output_dir="output",
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    eval_steps=100,
    save_steps=100,
    warmup_steps=0,
    logging_dir="logs",
    evaluation_strategy="steps",
    save_total_limit=3,
)

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=False,
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
)

# Save the fine-tuned model
model.save_pretrained("fine_tuned_mentalhealth_gpt2")

# Load the fine-tuned model
fine_tuned_model = GPT2LMHeadModel.from_pretrained("fine_tuned_mentalhealth_gpt2")

def ask_question(question, model, tokenizer, max_length=128, num_return_sequences=1):
    prompt = f"Question: {question}\nAnswer:"
    inputs = tokenizer.encode(prompt, return_tensors="pt", add_special_tokens=True)
    outputs = model.generate(
        inputs,
        max_length=max_length,
        num_return_sequences=num_return_sequences,
        no_repeat_ngram_size=3,
        do_sample=True,
        temperature=1.0,
        top_k=50,
        top_p=0.9,
        early_stopping=True,
    )

    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    answer = answer.replace(prompt, "").strip()

    # Truncate the answer after the first newline character
    answer = answer.split("\n")[0]

    return answer


# Ask questions using the fine-tuned model
question = "What is mental health"
answer = ask_question(question, fine_tuned_model, tokenizer)
print(f"Question: {question}\nAnswer: {answer}")

question = "Do I prefer yoga for depression and stress"
answer = ask_question(question, fine_tuned_model, tokenizer)
print(f"Question: {question}\nAnswer: {answer}")

question = "How to deal with mental disorder person"
answer = ask_question(question, fine_tuned_model, tokenizer)
print(f"Question: {question}\nAnswer: {answer}")

question = "what can I do to relief from smoking"
answer = ask_question(question, fine_tuned_model, tokenizer)
print(f"Question: {question}\nAnswer: {answer}")

question = "How to deal with drug addict person"
answer = ask_question(question, fine_tuned_model, tokenizer)
print(f"Question: {question}\nAnswer: {answer}")

question = "How to deal with anger person"
answer = ask_question(question, fine_tuned_model, tokenizer)
print(f"Question: {question}\nAnswer: {answer}")

!pip install pyTelegramBotAPI
!pip install openai

import telebot
import os
import openai

TelegramBOT_TOKEN = '5831284634:AAEug2B42onUbQwbrIHSprcYqz8UHDV6CgQ'
#Add API here
openai.api_key = "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"



bot = telebot.TeleBot(TelegramBOT_TOKEN)
@bot.message_handler(commands=['start', 'help'])
def send_welcome(message):
    bot.reply_to(message, "Welcome! The MOST POWERFUL AI BOT")

@bot.message_handler(func=lambda message: True)
def echo_all(message):
    print(message)
    response = openai.Completion.create(
    model="text-davinci-003",
    prompt=message.text,
    temperature=0.19,
    max_tokens=500,
    top_p=1,
    frequency_penalty=0,
    presence_penalty=0
    )
    bot.reply_to(message, response.choices[0].text)
bot.polling()